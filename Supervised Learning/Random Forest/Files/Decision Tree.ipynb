{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Decision Tree.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMZnManHz+VHM8LAKqdGtTE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SebleoHEQPHf"},"source":["# Import the needed libraries"]},{"cell_type":"code","metadata":{"id":"zJZ35C9BQWLW","executionInfo":{"status":"ok","timestamp":1636291008994,"user_tz":-120,"elapsed":358,"user":{"displayName":"Ofir Gilad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVQ5-KDUcXOndwUIl3ITIftawvt4VjCyo0uDRZ=s64","userId":"11248672105255574895"}}},"source":["import numpy as np\n","import pandas as pd"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0d56EZfbnn4_"},"source":["# Entropy Implementation"]},{"cell_type":"code","metadata":{"id":"GJHitEM3nqEj","executionInfo":{"status":"ok","timestamp":1636291009345,"user_tz":-120,"elapsed":7,"user":{"displayName":"Ofir Gilad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVQ5-KDUcXOndwUIl3ITIftawvt4VjCyo0uDRZ=s64","userId":"11248672105255574895"}}},"source":["def entropy(class_y):\n","    \"\"\" \n","    Input: \n","        - class_y: list of class labels (0's and 1's)\n","    \n","    Output: \n","        - The entropy\n","        \n","    Compute the entropy for a list of classes\n","    \"\"\"\n","    \n","    # Handling if there is only 1 or 0 labels\n","    if len(class_y) <= 1: \n","        return 0\n","    \n","    # count\n","    total_count = np.bincount(class_y)\n","    # Find the probabilities\n","    probabilities = total_count[np.nonzero(total_count)] / len(class_y) \n","\n","    # Handling if the length of the probabilities is less than or equal to 1\n","    if len(probabilities) <= 1 : \n","        return 0\n","    \n","    # Entropy equation\n","    return - np.sum(probabilities * np.log(probabilities)) / np.log(len(probabilities)) "],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I--dZny5oIwH"},"source":["# Information Gain"]},{"cell_type":"code","metadata":{"id":"OWCTxhU6oOvc","executionInfo":{"status":"ok","timestamp":1636291009346,"user_tz":-120,"elapsed":7,"user":{"displayName":"Ofir Gilad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVQ5-KDUcXOndwUIl3ITIftawvt4VjCyo0uDRZ=s64","userId":"11248672105255574895"}}},"source":["def information_gain(previous_y, current_y):\n","    \"\"\"\n","    Inputs:\n","        - previous_y : the distribution of original labels (0's and 1's)\n","        - current_y  : the distribution of labels after splitting based on a particular\n","                     split attribute and split value\n","    \n","    Outputs:\n","        - info_gain : The information gain after partitioning\n","        \n","    Compute and return the information gain from partitioning the previous_y labels into the current_y labels.\n","    \n","    \"\"\" \n","    \n","    # Y = previous_y\n","    # X = current_y\n","    # IG = H(Y) - CE(Y|X)\n","    conditional_entropy = 0 \n","    for y in current_y:\n","        conditional_entropy += (entropy(y)*len(y)/len(previous_y))\n","\n","    info_gain = entropy(previous_y) - conditional_entropy\n","    return info_gain"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FtIN3ENcrywx"},"source":["# The Decision Tree ID3 algorithm\n","1. Find best feature\n","2. Find best split for the best feature\n","3. Parition classes based on steps 1 and 2"]},{"cell_type":"markdown","metadata":{"id":"uRcu8MURr1bv"},"source":["# Step 3 : Partition Classes"]},{"cell_type":"code","metadata":{"id":"BLP15z0lr49M","executionInfo":{"status":"ok","timestamp":1636291009346,"user_tz":-120,"elapsed":7,"user":{"displayName":"Ofir Gilad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVQ5-KDUcXOndwUIl3ITIftawvt4VjCyo0uDRZ=s64","userId":"11248672105255574895"}}},"source":["def partition_classes(X, y, split_attribute, split_val):\n","    \"\"\"\n","    Inputs:\n","    - X               : (N,D) list containing all data attributes\n","    - y               : a list of labels\n","    - split_attribute : column index of the attribute to split on\n","    - split_val       : either a numerical or categorical value to divide the split_attribute\n","    \n","    Outputs:\n","    - X_left          : X left after partitioning\n","    - X_right         : X right after partitioning\n","    - y_left          : y left after partitioning\n","    - y_right         : y right after partitioning\n","    \n","    Partition the data(X) and labels(y) based on the split value    \n","    \"\"\"\n","    \"\"\"\n","    Example:\n","    \n","    X = [[3, 'aa', 10],                 y = [1,\n","         [1, 'bb', 22],                      1,\n","         [2, 'cc', 28],                      0,\n","         [5, 'bb', 32],                      0,\n","         [4, 'cc', 32]]                      1]\n","    \n","    Here, columns 0 and 2 represent numeric attributes, while column 1 is a categorical attribute.\n","    \n","    Consider the case where we call the function with split_attribute = 0 (the index of attribute) and split_val = 3 (the value of attribute).\n","    Then we divide X into two lists - X_left, where column 0 is <= 3 and X_right, where column 0 is > 3.\n","    \n","    X_left = [[3, 'aa', 10],                 y_left = [1,\n","              [1, 'bb', 22],                           1,\n","              [2, 'cc', 28]]                           0]\n","              \n","    X_right = [[5, 'bb', 32],                y_right = [0,\n","               [4, 'cc', 32]]                           1]\n","\n","    Consider another case where we call the function with split_attribute = 1 and split_val = 'bb'\n","    Then we divide X into two lists, one where column 1 is 'bb', and the other where it is not 'bb'.\n","        \n","    X_left = [[1, 'bb', 22],                 y_left = [1,\n","              [5, 'bb', 32]]                           0]\n","              \n","    X_right = [[3, 'aa', 10],                y_right = [1,\n","               [2, 'cc', 28],                           0,\n","               [4, 'cc', 32]]                           1]\n","               \n","    \"\"\"\n","    \n","    X = np.array(X)\n","    column_split = X[:, split_attribute]\n","    X_left = []\n","    y_right = []\n","    X_right = []\n","    y_left = []\n","    \n","    # Counter for appending\n","    counter = 0\n","\n","    # Numerical attribute\n","    if isinstance(split_val,str) == False: \n","        for i in column_split:\n","            if i <= split_val:\n","                X_left.append(X[counter])\n","                y_left.append(y[counter])\n","            else:\n","                X_right.append(X[counter])\n","                y_right.append(y[counter])\n","            counter += 1\n","\n","    # Categorical attribute    \n","    else: \n","        for i in column_split:\n","            # == and NOT <=\n","            if i == split_val:\n","                X_left.append(X[counter])\n","                y_left.append(y[counter])\n","            else:\n","                X_right.append(X[counter])\n","                y_right.append(y[counter])\n","            counter += 1\n","\n","    return X_left, X_right, y_left, y_right "],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J8mtZ_IRui5Y"},"source":["# Step 2 : Find Best Split"]},{"cell_type":"code","metadata":{"id":"JCYFDUNMujkj","executionInfo":{"status":"ok","timestamp":1636291009347,"user_tz":-120,"elapsed":7,"user":{"displayName":"Ofir Gilad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVQ5-KDUcXOndwUIl3ITIftawvt4VjCyo0uDRZ=s64","userId":"11248672105255574895"}}},"source":["def find_best_split(X, y, split_attribute):\n","    \n","    \"\"\"\n","    Inputs:\n","        - X               : (N,D) list containing all data attributes\n","        - y               : a list array of labels\n","        - split_attribute : Column of X on which to split\n","    \n","    Outputs:\n","        - best_split_val  : optimal split value for a given attribute\n","        - best_info_gain  : the corresponding information gain\n","    \n","    Compute and return the optimal split value for a given attribute, along with the corresponding information gain\n","\n","    \"\"\"\n","    \n","    \"\"\"\n","    \n","      Example:\n","    \n","        X = [[3, 'aa', 10],                 y = [1,\n","             [1, 'bb', 22],                      1,\n","             [2, 'cc', 28],                      0,\n","             [5, 'bb', 32],                      0,\n","             [4, 'cc', 32]]                      1]\n","    \n","        split_attribute = 0\n","        \n","        Starting entropy: 0.971\n","        \n","        Calculate information gain at splits:\n","           split_val = 1  -->  info_gain = 0.17\n","           split_val = 2  -->  info_gain = 0.01997\n","           split_val = 3  -->  info_gain = 0.01997\n","           split_val = 4  -->  info_gain = 0.32\n","           split_val = 5  -->  info_gain = 0\n","        \n","       best_split_val = 4; info_gain = .32; \n","    \"\"\"\n","    \n","    # Initialize the best information gain\n","    best_info_gain = 0 \n","    X = np.array(X)\n","    column_split = X[:,split_attribute]\n","\n","    # Get the unique values only !\n","    column_split = np.unique(column_split)\n","    # Set the best split as the initial one (in case it is the only one)\n","    best_split_val = column_split[0] \n","\n","    for split_val in column_split:\n","        # Call the partition classes function\n","        current_X_left, current_X_right, current_y_left, current_y_right = partition_classes(X, y, split_attribute, split_val)\n","        current_y = []\n","        current_y.append(current_y_left)\n","        current_y.append(current_y_right)\n","\n","        # Calculate the information gain\n","        current_info_gain = information_gain(y,current_y)\n","        # check if it is better than before\n","        if current_info_gain > best_info_gain:\n","            best_info_gain = current_info_gain\n","            best_split_val = split_val\n","\n","    return best_split_val, best_info_gain"],"execution_count":45,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6GwAuhEz1Tgc"},"source":["# Step 1 : Find Best Feature"]},{"cell_type":"code","metadata":{"id":"dfmlXe5p1UJM","executionInfo":{"status":"ok","timestamp":1636291009348,"user_tz":-120,"elapsed":7,"user":{"displayName":"Ofir Gilad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVQ5-KDUcXOndwUIl3ITIftawvt4VjCyo0uDRZ=s64","userId":"11248672105255574895"}}},"source":["def find_best_feature(X, y):\n","    \"\"\"\n","    Inputs:\n","        - X              : (N,D) list containing all data attributes\n","        - y              : a list of labels\n","    \n","    Outputs:\n","        - best_feature   : Best Feature to split on\n","        - best_split_val : Best Split Value for this feature\n","        \n","    Compute and return the optimal attribute to split on and optimal splitting value\n","    \"\"\"\n","    \n","    \"\"\"\n","        \n","    Example:\n","    \n","        X = [[3, 'aa', 10],                 y = [1,\n","             [1, 'bb', 22],                      1,\n","             [2, 'cc', 28],                      0,\n","             [5, 'bb', 32],                      0,\n","             [4, 'cc', 32]]                      1]\n","    \n","        split_attribute = 0\n","        \n","        Starting entropy: 0.971\n","        \n","        Calculate information gain at splits:\n","           feature 0:  -->  info_gain = 0.32\n","           feature 1:  -->  info_gain = 0.17\n","           feature 2:  -->  info_gain = 0.4199\n","        \n","       best_split_feature: 2 best_split_val: 22\n","    \"\"\"\n","    \n","    \n","    best_info_gain = 0\n","    best_feature = 0\n","    best_split_val = 0\n","    # Loop over the features , find the best split \n","    for feature_index in range(len(X[0])):\n","        current_best_split_val, current_best_info_gain =  find_best_split(X, y,  feature_index)\n","        if current_best_info_gain > best_info_gain:\n","            best_info_gain = current_best_info_gain\n","            best_feature = feature_index\n","            best_split_val = current_best_split_val\n","    return best_feature, best_split_val"],"execution_count":46,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Ob-fKjPGlv9"},"source":["# Decision Tree ID3 Full Algorithm : Putting Everything Together"]},{"cell_type":"code","metadata":{"id":"a62Y6_Y8GmNw","executionInfo":{"status":"ok","timestamp":1636291009348,"user_tz":-120,"elapsed":7,"user":{"displayName":"Ofir Gilad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjVQ5-KDUcXOndwUIl3ITIftawvt4VjCyo0uDRZ=s64","userId":"11248672105255574895"}}},"source":["class MyDecisionTree(object):\n","    def __init__(self, max_depth=None):\n","        \"\"\"        \n","        Args:\n","        \n","        max_depth: maximum depth of the tree including the root node.\n","        \"\"\"\n","        self.tree = {}\n","        # For prediction\n","        self.residual_tree = {}\n","        self.max_depth = max_depth\n","\n","        \n","    def fit(self, X, y, depth):\n","        \"\"\"\n","        Args:\n","        X     : N*D matrix corresponding to the data points\n","        Y     : N*1 array corresponding to the labels of the data points\n","        depth : depth of node of the tree\n","        \n","        Output:\n","        node : A dictionary that contains this node information\n","        \"\"\"\n","\n","        # Base cases\n","        unique_labels = np.unique(y) \n","        # If we reach the maximum depth or the unique labels = 1\n","        if (len(unique_labels) == 1) or (depth == self.max_depth):\n","            unique_labels, counts_unique_labels = np.unique(y, return_counts=True)\n","            index = counts_unique_labels.argmax()\n","            classification = unique_labels[index]    \n","            return classification\n","    \n","        # Find best feature (Step 1)\n","        best_feat, best_split  = find_best_feature(X, y)\n","        # Split on this feature (Step 2) - (Redundant)\n","        best_split, information_gain = find_best_split(X, y, best_feat)\n","        # Partition on the best feature and split (Step 3)\n","        X_left, X_right, y_left, y_right = partition_classes(X, y, best_feat, best_split)\n","        \n","        if isinstance(best_split,str):\n","            # Represnt the sub-tree as a question and an answer\n","            question = \"{} == {}\".format(best_feat, best_split)\n","        else:\n","            # Represnt the sub-tree as a question and an answer\n","            question = \"{} <= {}\".format(best_feat, best_split)\n","        node = {question: []}\n","        \n","        # Find answers (recursion)\n","        depth+=1 \n","        # RECURSION on the left sub tree\n","        yes_answer = self.fit(X_left,y_left, depth)\n","        # RECURSION on the right sub tree\n","        no_answer = self.fit(X_right, y_right, depth)\n","        \n","        # Both trees are the same\n","        if yes_answer == no_answer:\n","            node = yes_answer\n","        else:\n","            # Append the questions to the answers\n","            node[question].append(yes_answer)\n","            node[question].append(no_answer)\n","        # The tree is equal to the node\n","        self.tree = node\n","        return node\n","        \n","    def predict(self, record,flag=1):\n","        \"\"\"\n","        Args:\n","        \n","        record: D*1, a single data point that should be classified\n","        \n","        Output:\n","        prediction: True if the predicted class label is 1, False otherwise      \n","        \n","        classify a sample in test data set using self.tree and return the predicted label\n","        \"\"\"\n","        # First time\n","        if flag == 1:\n","            self.residual_tree = self.tree\n","        question = list(self.residual_tree.keys())[0]\n","        # Split the question to get the feature and its value\n","        feature, comparison, value = question.split()\n","\n","        \n","        # Check if str or int\n","        # String\n","        if comparison == \"==\": \n","            if record[int(feature)] == value:\n","                # Left sub tree\n","                answer = self.residual_tree[question][0]\n","            else:\n","                # Right sub-tree\n","                answer = self.residual_tree[question][1]\n","        # Integer\n","        elif comparison == \"<=\": \n","            if record[int(feature)] <= float(value):\n","                # Right Sub-tree\n","                answer = self.residual_tree[question][0]\n","            else:\n","                # Left sub-tree\n","                answer = self.residual_tree[question][1]\n","\n","        # Base case\n","        # If we have the answer\n","        if not isinstance(answer, dict):\n","            return answer\n","    \n","        # Recursion\n","        else:\n","            # The residual tree is the answer!\n","            self.residual_tree = answer\n","            # have flag = 0 so the residual tree is our sub-problem\n","            return self.predict(record,0)"],"execution_count":47,"outputs":[]}]}